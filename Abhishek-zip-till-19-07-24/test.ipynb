{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (0.2.6)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (0.1.13)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (0.2.6)\n",
      "Requirement already satisfied: datasets in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (2.20.0)\n",
      "Requirement already satisfied: streamlit in c:\\programdata\\anaconda3\\lib\\site-packages (1.30.0)\n",
      "Requirement already satisfied: ragas in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (0.1.9)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (3.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain_openai) (1.35.7)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain_community) (0.6.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (7.0.0)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (4.9.0)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (2.1)\n",
      "Requirement already satisfied: validators<1,>=0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (0.18.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\programdata\\anaconda3\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\programdata\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from validators<1,>=0.2->streamlit) (5.1.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abhisheka\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (2.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain_openai langchain_community datasets streamlit ragas PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate \n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = r\"C:\\Users\\abhisheka\\OneDrive - USEReady Technology Private Limited\\Desktop\\RAGAS\\Streamlit with Databricks.pdf\"\n",
    "\n",
    "if pdf is not None:\n",
    "    pdf_reader = PdfReader(pdf)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking, Embedding and Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"\"\" you are a helpful pdf assistant. \n",
    "        Given the following pdf, answer the question based on the context.\n",
    "        If you don't know the answer, just say that you don't know. \n",
    "        Do not make up an answer.\n",
    "\n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    |StrOutputParser()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhisheka\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "questions = [\"what are the common uses of databricks?\",\n",
    "                \"when can we integrate databricks into streamlit?\"]\n",
    "\n",
    "ground_truths=[[\"Databricks is commonly used for big data analytics and processing, particularly for Apache Spark-based data processing tasks. It provides a unified analytics platform that integrates with various data sourcesand supports collaborative data science workflows. Additionally, it's utilized for machine learning, data engineering, and real-time analytics applications.\"],\n",
    "                [\"We can integrate Databricks into your Streamlit app for data processing tasks by leveraging Databricks as a backend service. For example, you can use Databricks for heavy-duty data transformations, machine learning model training, or large-scale data analysis tasks.\"]]\n",
    "\n",
    "\n",
    "answers=[]\n",
    "contexts=[]\n",
    "\n",
    "for query in questions:\n",
    "    answers.append(rag_chain.invoke(query))\n",
    "    contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"STREAMLIT\\nAPPS WITH \\nDATABRICKS \\nML apps that can have\\npowerful backend support\\nwith DatabricksCOMMON USES OF \\nDATABRICKS\\n•Databricks is commonly used for big data analytics and \\nprocessing, particularly for Apache Spark-based data \\nprocessing tasks. It provides a unified analytics platform \\nthat integrates with various data sources and supports \\ncollaborative data science workflows. Additionally, it's \\nutilized for machine learning, data engineering, and \\nreal-time analytics applications.\",\n",
       "  \"1. Data Preprocessing : Databricks can handle the preprocessing of PDF files, \\nextracting text and relevant information from them efficiently at scale.\\n2. Data Analysis : You can use Databricks to analyze the extracted text data, \\nperform natural language processing tasks, and extract insights to aid in the \\nscreening process.\\n3. Model Training : If you're using machine learning models, Databricks can be \\nused to train these models on large datasets, optimizing performance and \\nscalability.\",\n",
       "  \"like Heroku or AWS for better performance and scalability. \\nDatabricks could be integrated into your app for data \\nprocessing tasks if needed, but it's not typically used for \\nhosting web applications.\\nINTEGRATING \\nDATABRICKS INTO \\nSTREAMLIT\\n•We can integrate Databricks into your \\nStreamlit app for data processing tasks by \\nleveraging Databricks as a backend service. \\nFor example, you can use Databricks for \\nheavy-duty data transformations, machine \\nlearning model training, or large-scale data\",\n",
       "  \"which is optimized for big data processing, \\nwhile still providing a user-friendly interface \\nthrough Streamlit for interacting with and \\nvisualizing the results of these tasks. It's a \\npowerful combination that combines the \\nstrengths of both platforms for a seamless \\ndata processing and analysis experience.\\nHOW WILL DATABRICKS \\nPLAY A ROLE\\n•Databricks can play a role in several aspects of your resume screener app:\\n1. Data Preprocessing : Databricks can handle the preprocessing of PDF files,\"],\n",
       " ['heavy-duty data transformations, machine \\nlearning model training, or large-scale data \\nanalysis tasks. Your Streamlit app can \\ncommunicate with Databricks through its \\nREST API or by using Databricks libraries \\ndirectly within your Python code.\\nINTEGRATING \\nDATABRICKS INTO \\nSTREAMLIT\\nThis integration allows you to offload \\nresource-intensive tasks to Databricks, \\nwhich is optimized for big data processing, \\nwhile still providing a user-friendly interface',\n",
       "  \"like Heroku or AWS for better performance and scalability. \\nDatabricks could be integrated into your app for data \\nprocessing tasks if needed, but it's not typically used for \\nhosting web applications.\\nINTEGRATING \\nDATABRICKS INTO \\nSTREAMLIT\\n•We can integrate Databricks into your \\nStreamlit app for data processing tasks by \\nleveraging Databricks as a backend service. \\nFor example, you can use Databricks for \\nheavy-duty data transformations, machine \\nlearning model training, or large-scale data\",\n",
       "  'utilized for machine learning, data engineering, and \\nreal-time analytics applications.\\nIS HOSTING A STREAMLIT \\nAPP ON DATABRICKS A \\nGOOD IDEA?\\nRunning a Streamlit app like resume screener on \\nDatabricks might not be the best fit. Databricks is more \\nsuited for big data processing and analytics tasks, while \\nStreamlit is designed for building interactive web apps \\nwith Python. You can host your Streamlit app on platforms \\nlike Heroku or AWS for better performance and scalability.',\n",
       "  \"which is optimized for big data processing, \\nwhile still providing a user-friendly interface \\nthrough Streamlit for interacting with and \\nvisualizing the results of these tasks. It's a \\npowerful combination that combines the \\nstrengths of both platforms for a seamless \\ndata processing and analysis experience.\\nHOW WILL DATABRICKS \\nPLAY A ROLE\\n•Databricks can play a role in several aspects of your resume screener app:\\n1. Data Preprocessing : Databricks can handle the preprocessing of PDF files,\"]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['what are the common uses of databricks?',\n",
       "  'when can we integrate databricks into streamlit?'],\n",
       " 'answer': ['Common uses of Databricks include big data analytics and processing, particularly for Apache Spark-based data processing tasks. It also provides a unified analytics platform that integrates with various data sources and supports collaborative data science workflows. Additionally, Databricks is utilized for machine learning, data engineering, and real-time analytics applications.',\n",
       "  'You can integrate Databricks into Streamlit for data processing tasks by leveraging Databricks as a backend service.'],\n",
       " 'contexts': [[\"STREAMLIT\\nAPPS WITH \\nDATABRICKS \\nML apps that can have\\npowerful backend support\\nwith DatabricksCOMMON USES OF \\nDATABRICKS\\n•Databricks is commonly used for big data analytics and \\nprocessing, particularly for Apache Spark-based data \\nprocessing tasks. It provides a unified analytics platform \\nthat integrates with various data sources and supports \\ncollaborative data science workflows. Additionally, it's \\nutilized for machine learning, data engineering, and \\nreal-time analytics applications.\",\n",
       "   \"1. Data Preprocessing : Databricks can handle the preprocessing of PDF files, \\nextracting text and relevant information from them efficiently at scale.\\n2. Data Analysis : You can use Databricks to analyze the extracted text data, \\nperform natural language processing tasks, and extract insights to aid in the \\nscreening process.\\n3. Model Training : If you're using machine learning models, Databricks can be \\nused to train these models on large datasets, optimizing performance and \\nscalability.\",\n",
       "   \"like Heroku or AWS for better performance and scalability. \\nDatabricks could be integrated into your app for data \\nprocessing tasks if needed, but it's not typically used for \\nhosting web applications.\\nINTEGRATING \\nDATABRICKS INTO \\nSTREAMLIT\\n•We can integrate Databricks into your \\nStreamlit app for data processing tasks by \\nleveraging Databricks as a backend service. \\nFor example, you can use Databricks for \\nheavy-duty data transformations, machine \\nlearning model training, or large-scale data\",\n",
       "   \"which is optimized for big data processing, \\nwhile still providing a user-friendly interface \\nthrough Streamlit for interacting with and \\nvisualizing the results of these tasks. It's a \\npowerful combination that combines the \\nstrengths of both platforms for a seamless \\ndata processing and analysis experience.\\nHOW WILL DATABRICKS \\nPLAY A ROLE\\n•Databricks can play a role in several aspects of your resume screener app:\\n1. Data Preprocessing : Databricks can handle the preprocessing of PDF files,\"],\n",
       "  ['heavy-duty data transformations, machine \\nlearning model training, or large-scale data \\nanalysis tasks. Your Streamlit app can \\ncommunicate with Databricks through its \\nREST API or by using Databricks libraries \\ndirectly within your Python code.\\nINTEGRATING \\nDATABRICKS INTO \\nSTREAMLIT\\nThis integration allows you to offload \\nresource-intensive tasks to Databricks, \\nwhich is optimized for big data processing, \\nwhile still providing a user-friendly interface',\n",
       "   \"like Heroku or AWS for better performance and scalability. \\nDatabricks could be integrated into your app for data \\nprocessing tasks if needed, but it's not typically used for \\nhosting web applications.\\nINTEGRATING \\nDATABRICKS INTO \\nSTREAMLIT\\n•We can integrate Databricks into your \\nStreamlit app for data processing tasks by \\nleveraging Databricks as a backend service. \\nFor example, you can use Databricks for \\nheavy-duty data transformations, machine \\nlearning model training, or large-scale data\",\n",
       "   'utilized for machine learning, data engineering, and \\nreal-time analytics applications.\\nIS HOSTING A STREAMLIT \\nAPP ON DATABRICKS A \\nGOOD IDEA?\\nRunning a Streamlit app like resume screener on \\nDatabricks might not be the best fit. Databricks is more \\nsuited for big data processing and analytics tasks, while \\nStreamlit is designed for building interactive web apps \\nwith Python. You can host your Streamlit app on platforms \\nlike Heroku or AWS for better performance and scalability.',\n",
       "   \"which is optimized for big data processing, \\nwhile still providing a user-friendly interface \\nthrough Streamlit for interacting with and \\nvisualizing the results of these tasks. It's a \\npowerful combination that combines the \\nstrengths of both platforms for a seamless \\ndata processing and analysis experience.\\nHOW WILL DATABRICKS \\nPLAY A ROLE\\n•Databricks can play a role in several aspects of your resume screener app:\\n1. Data Preprocessing : Databricks can handle the preprocessing of PDF files,\"]],\n",
       " 'ground_truths': [[\"Databricks is commonly used for big data analytics and processing, particularly for Apache Spark-based data processing tasks. It provides a unified analytics platform that integrates with various data sourcesand supports collaborative data science workflows. Additionally, it's utilized for machine learning, data engineering, and real-time analytics applications.\"],\n",
       "  ['We can integrate Databricks into your Streamlit app for data processing tasks by leveraging Databricks as a backend service. For example, you can use Databricks for heavy-duty data transformations, machine learning model training, or large-scale data analysis tasks.']]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f23697ad674b5e9f4f2d9c15cd4ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[context_precision, context_recall, faithfulness, answer_relevancy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['faithfulness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what are the common uses of databricks?</td>\n",
       "      <td>Common uses of Databricks include big data ana...</td>\n",
       "      <td>[STREAMLIT\\nAPPS WITH \\nDATABRICKS \\nML apps t...</td>\n",
       "      <td>[Databricks is commonly used for big data anal...</td>\n",
       "      <td>Databricks is commonly used for big data analy...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when can we integrate databricks into streamlit?</td>\n",
       "      <td>You can integrate Databricks into Streamlit fo...</td>\n",
       "      <td>[heavy-duty data transformations, machine \\nle...</td>\n",
       "      <td>[We can integrate Databricks into your Streaml...</td>\n",
       "      <td>We can integrate Databricks into your Streamli...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.929206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question  \\\n",
       "0           what are the common uses of databricks?   \n",
       "1  when can we integrate databricks into streamlit?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Common uses of Databricks include big data ana...   \n",
       "1  You can integrate Databricks into Streamlit fo...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [STREAMLIT\\nAPPS WITH \\nDATABRICKS \\nML apps t...   \n",
       "1  [heavy-duty data transformations, machine \\nle...   \n",
       "\n",
       "                                       ground_truths  \\\n",
       "0  [Databricks is commonly used for big data anal...   \n",
       "1  [We can integrate Databricks into your Streaml...   \n",
       "\n",
       "                                        ground_truth  context_precision  \\\n",
       "0  Databricks is commonly used for big data analy...                1.0   \n",
       "1  We can integrate Databricks into your Streamli...                1.0   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             1.0           1.0          0.977396  \n",
       "1             1.0           1.0          0.929206  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
